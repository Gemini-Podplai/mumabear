"""
üöÄ Express Mode + Vertex AI Integration Service
6x faster responses via Google Cloud Vertex AI with Claude model access
Maintains existing Gemini API setup while adding premium Claude capabilities
"""

import asyncio
import logging
import time
import json
import uuid
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, field
from enum import Enum

import aiohttp
from google.cloud import aiplatform
from google.auth import default
import google.auth.transport.requests

logger = logging.getLogger(__name__)

class ExecutionMode(Enum):
    """Express Mode execution strategies"""
    EXPRESS = "express"  # 6x faster via Vertex AI
    STANDARD = "standard"  # Existing Gemini API
    PREMIUM = "premium"  # Claude models via Vertex AI
    SMART_ROUTING = "smart_routing"  # Intelligent selection

class ModelProvider(Enum):
    """Model providers for smart routing"""
    GEMINI_API = "gemini_api"  # Direct Gemini API (existing)
    VERTEX_GEMINI = "vertex_gemini"  # Gemini via Vertex AI
    VERTEX_CLAUDE = "vertex_claude"  # Claude via Vertex AI

@dataclass
class ExpressConfig:
    """Configuration for Express Mode execution"""
    mode: ExecutionMode = ExecutionMode.SMART_ROUTING
    max_response_time_ms: int = 2000  # 6x faster than standard
    enable_streaming: bool = True
    parallel_processing: bool = True
    cache_enabled: bool = True
    smart_routing: bool = True

@dataclass
class VertexModelConfig:
    """Vertex AI model configuration"""
    model_name: str
    provider: ModelProvider
    region: str = "us-central1"
    project_id: str = None
    endpoint_id: str = None
    max_tokens: int = 8192
    temperature: float = 0.7
    top_p: float = 0.95
    top_k: int = 40
    
    # Performance characteristics
    avg_response_time_ms: int = 1000
    cost_per_1k_tokens: float = 0.001
    requests_per_minute: int = 60
    
    # Health tracking
    consecutive_errors: int = 0
    last_error: Optional[str] = None
    is_healthy: bool = True
    last_health_check: Optional[datetime] = None

@dataclass
class ExpressResponse:
    """Response from Express Mode execution"""
    content: str
    model_used: str
    provider: ModelProvider
    execution_mode: ExecutionMode
    processing_time_ms: int
    tokens_used: Dict[str, int]
    cost_estimate: float
    cache_hit: bool = False
    streaming_enabled: bool = False
    fallback_count: int = 0
    
    # Agentic metadata
    reasoning_steps: List[str] = field(default_factory=list)
    confidence_score: float = 1.0
    autonomous_actions: List[str] = field(default_factory=list)

class ExpressModeVertexIntegration:
    """
    üöÄ Express Mode + Vertex AI Integration
    
    Provides:
    - 6x faster responses via Vertex AI optimizations
    - Access to Claude 4/3.5/3.7 models via Google Cloud credits
    - Smart routing between Gemini API and Vertex AI
    - Agentic control capabilities for Mama Bear
    """
    
    def __init__(self, project_id: str, region: str = "us-central1", 
                 credentials_path: str = None):
        self.project_id = project_id
        self.region = region
        self.express_config = ExpressConfig()
        
        # Initialize Google Cloud credentials
        self.credentials, _ = default()
        self.auth_request = google.auth.transport.requests.Request()
        
        # Initialize Vertex AI client
        aiplatform.init(project=project_id, location=region)
        
        # Model configurations
        self.vertex_models = self._initialize_vertex_models()
        self.model_performance_cache = {}
        self.response_cache = {}
        
        # Agentic control state
        self.autonomous_mode = False
        self.agent_creation_requests = []
        self.infrastructure_changes = []
        
        logger.info("üöÄ Express Mode + Vertex AI Integration initialized!")
    
    def _initialize_vertex_models(self) -> Dict[str, VertexModelConfig]:
        """Initialize available Vertex AI models"""
        return {
            # Claude Models via Vertex AI Model Garden
            "claude-4-opus": VertexModelConfig(
                model_name="claude-opus-4@20250105",
                provider=ModelProvider.VERTEX_CLAUDE,
                avg_response_time_ms=800,
                cost_per_1k_tokens=0.015,  # Premium pricing
                requests_per_minute=30
            ),
            "claude-4-sonnet": VertexModelConfig(
                model_name="claude-sonnet-4@20250105", 
                provider=ModelProvider.VERTEX_CLAUDE,
                avg_response_time_ms=600,
                cost_per_1k_tokens=0.003,
                requests_per_minute=60
            ),
            "claude-3.7-sonnet": VertexModelConfig(
                model_name="claude-3-7-sonnet@20241022",
                provider=ModelProvider.VERTEX_CLAUDE,
                avg_response_time_ms=500,
                cost_per_1k_tokens=0.003,
                requests_per_minute=60
            ),
            "claude-3.5-sonnet-v2": VertexModelConfig(
                model_name="claude-3-5-sonnet-v2@20241022",
                provider=ModelProvider.VERTEX_CLAUDE,
                avg_response_time_ms=400,
                cost_per_1k_tokens=0.003,
                requests_per_minute=60
            ),
            "claude-3.5-haiku": VertexModelConfig(
                model_name="claude-3-5-haiku@20241022",
                provider=ModelProvider.VERTEX_CLAUDE,
                avg_response_time_ms=300,  # Fastest Claude model
                cost_per_1k_tokens=0.00025,  # Most cost-effective
                requests_per_minute=120
            ),
            
            # Gemini Models via Vertex AI (Express Mode)
            "gemini-2.5-pro-vertex": VertexModelConfig(
                model_name="gemini-2.5-pro-preview-0611",
                provider=ModelProvider.VERTEX_GEMINI,
                avg_response_time_ms=400,
                cost_per_1k_tokens=0.00125,
                requests_per_minute=120
            ),
            "gemini-2.5-flash-vertex": VertexModelConfig(
                model_name="gemini-2.5-flash-preview-0611",
                provider=ModelProvider.VERTEX_GEMINI,
                avg_response_time_ms=200,  # Fastest option
                cost_per_1k_tokens=0.000075,  # Most cost-effective
                requests_per_minute=300
            )
        }
    
    async def process_express_request(self, request: Dict[str, Any]) -> ExpressResponse:
        """
        üöÄ Process request via Express Mode with intelligent routing
        """
        start_time = time.time()
        request_id = str(uuid.uuid4())
        
        # Extract request details
        message = request.get("message", "")
        variant = request.get("variant", "scout_commander")
        context = request.get("context", {})
        user_id = request.get("user_id", "default")
        execution_mode = ExecutionMode(request.get("mode", "smart_routing"))
        
        logger.info(f"üöÄ Processing Express Mode request: {message[:100]}...")
        
        try:
            # Step 1: Analyze request for optimal routing
            routing_decision = await self._analyze_for_routing(message, context, execution_mode)
            
            # Step 2: Execute via optimal model/provider
            if routing_decision["provider"] == ModelProvider.VERTEX_CLAUDE:
                response = await self._execute_claude_request(message, routing_decision, context)
            elif routing_decision["provider"] == ModelProvider.VERTEX_GEMINI:
                response = await self._execute_vertex_gemini_request(message, routing_decision, context)
            else:
                # Fallback to existing Gemini API
                response = await self._execute_gemini_api_fallback(message, context)
            
            # Step 3: Apply Express Mode optimizations
            if execution_mode == ExecutionMode.EXPRESS:
                response = await self._apply_express_optimizations(response)
            
            # Step 4: Handle agentic capabilities if enabled
            if self.autonomous_mode:
                response = await self._apply_agentic_control(response, context)
            
            processing_time = int((time.time() - start_time) * 1000)
            
            return ExpressResponse(
                content=response.get("content", ""),
                model_used=routing_decision["model"],
                provider=routing_decision["provider"],
                execution_mode=execution_mode,
                processing_time_ms=processing_time,
                tokens_used=response.get("tokens", {"input": 0, "output": 0}),
                cost_estimate=response.get("cost", 0.0),
                cache_hit=response.get("cache_hit", False),
                streaming_enabled=self.express_config.enable_streaming,
                reasoning_steps=response.get("reasoning", []),
                confidence_score=response.get("confidence", 1.0),
                autonomous_actions=response.get("autonomous_actions", [])
            )
            
        except Exception as e:
            logger.error(f"‚ùå Express Mode request failed: {e}")
            
            # Graceful fallback
            fallback_response = await self._generate_express_fallback(message, str(e))
            processing_time = int((time.time() - start_time) * 1000)
            
            return ExpressResponse(
                content=fallback_response,
                model_used="fallback",
                provider=ModelProvider.GEMINI_API,
                execution_mode=ExecutionMode.STANDARD,
                processing_time_ms=processing_time,
                tokens_used={"input": 0, "output": 0},
                cost_estimate=0.0,
                fallback_count=1,
                confidence_score=0.5
            )
    
    async def _analyze_for_routing(self, message: str, context: Dict[str, Any], 
                                  mode: ExecutionMode) -> Dict[str, Any]:
        """
        üß† Intelligent routing analysis for optimal model selection
        """
        
        # Analyze request characteristics
        complexity_score = self._assess_complexity(message, context)
        urgency_level = self._assess_urgency(message, context)
        cost_sensitivity = context.get("cost_sensitivity", "balanced")
        quality_requirements = context.get("quality_requirements", "standard")
        
        # Smart routing logic
        if mode == ExecutionMode.EXPRESS:
            # Prioritize speed - use fastest available model
            if urgency_level == "high":
                return {
                    "model": "gemini-2.5-flash-vertex",
                    "provider": ModelProvider.VERTEX_GEMINI,
                    "reasoning": "Express mode with high urgency - using fastest model"
                }
            else:
                return {
                    "model": "claude-3.5-haiku",
                    "provider": ModelProvider.VERTEX_CLAUDE,
                    "reasoning": "Express mode - using fast, cost-effective Claude model"
                }
        
        elif mode == ExecutionMode.PREMIUM:
            # Prioritize quality - use best available model
            if complexity_score >= 8:
                return {
                    "model": "claude-4-opus",
                    "provider": ModelProvider.VERTEX_CLAUDE,
                    "reasoning": "Premium mode with high complexity - using Claude 4 Opus"
                }
            else:
                return {
                    "model": "claude-4-sonnet", 
                    "provider": ModelProvider.VERTEX_CLAUDE,
                    "reasoning": "Premium mode - using Claude 4 Sonnet for balanced quality"
                }
        
        elif mode == ExecutionMode.SMART_ROUTING:
            # Intelligent selection based on multiple factors
            
            # High complexity coding/agentic tasks -> Claude 3.7/4
            if (complexity_score >= 7 and 
                ("code" in message.lower() or "agent" in message.lower() or 
                 "complex" in message.lower())):
                return {
                    "model": "claude-3.7-sonnet",
                    "provider": ModelProvider.VERTEX_CLAUDE,
                    "reasoning": "Complex coding/agentic task - using Claude 3.7 Sonnet"
                }
            
            # Cost-sensitive + good quality -> Claude 3.5 Haiku
            elif cost_sensitivity == "high":
                return {
                    "model": "claude-3.5-haiku",
                    "provider": ModelProvider.VERTEX_CLAUDE,
                    "reasoning": "Cost-sensitive request - using Claude 3.5 Haiku"
                }
            
            # Speed requirements -> Gemini Flash via Vertex
            elif urgency_level == "high" or "fast" in message.lower():
                return {
                    "model": "gemini-2.5-flash-vertex",
                    "provider": ModelProvider.VERTEX_GEMINI,
                    "reasoning": "Speed requirement - using Gemini 2.5 Flash via Vertex"
                }
            
            # Balanced quality/cost -> Claude 3.5 Sonnet v2
            else:
                return {
                    "model": "claude-3.5-sonnet-v2",
                    "provider": ModelProvider.VERTEX_CLAUDE,
                    "reasoning": "Balanced quality/cost - using Claude 3.5 Sonnet v2"
                }
        
        else:
            # Standard mode - fallback to existing Gemini API
            return {
                "model": "gemini-2.5-pro-primary",
                "provider": ModelProvider.GEMINI_API,
                "reasoning": "Standard mode - using existing Gemini API"
            }
    
    def _assess_complexity(self, message: str, context: Dict[str, Any]) -> int:
        """Assess complexity score (1-10) based on message content"""
        complexity = 3  # Base complexity
        
        # Code-related indicators
        if any(keyword in message.lower() for keyword in 
               ["implement", "refactor", "debug", "algorithm", "architecture"]):
            complexity += 2
        
        # Agentic/AI indicators
        if any(keyword in message.lower() for keyword in 
               ["agent", "autonomous", "ai", "intelligence", "reasoning"]):
            complexity += 2
        
        # Multi-step task indicators
        if any(keyword in message.lower() for keyword in 
               ["plan", "strategy", "workflow", "process", "integrate"]):
            complexity += 1
        
        # Context complexity
        if len(str(context)) > 1000:
            complexity += 1
        
        return min(complexity, 10)
    
    def _assess_urgency(self, message: str, context: Dict[str, Any]) -> str:
        """Assess urgency level based on message content"""
        urgent_indicators = ["urgent", "fast", "quick", "asap", "immediate", "now"]
        
        if any(indicator in message.lower() for indicator in urgent_indicators):
            return "high"
        elif any(indicator in message.lower() for indicator in ["slow", "careful", "detailed"]):
            return "low"
        else:
            return "normal"
    
    async def _execute_claude_request(self, message: str, routing: Dict[str, Any], 
                                    context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ü§ñ Execute request via Claude models on Vertex AI
        """
        model_config = self.vertex_models[routing["model"]]
        
        # Build Claude-compatible request
        request_data = {
            "anthropic_version": "vertex-2023-10-16",
            "messages": [
                {
                    "role": "user",
                    "content": message
                }
            ],
            "max_tokens": model_config.max_tokens,
            "temperature": model_config.temperature,
            "top_p": model_config.top_p,
            "stream": self.express_config.enable_streaming
        }
        
        # Add system message with Mama Bear personality if context provided
        if context.get("mama_bear_variant"):
            system_message = self._build_mama_bear_system_message(
                context["mama_bear_variant"], context
            )
            request_data["system"] = system_message
        
        try:
            # Execute via Vertex AI endpoint
            response = await self._call_vertex_endpoint(
                model_config.model_name, 
                request_data,
                "anthropic"
            )
            
            # Parse response
            content = self._extract_claude_content(response)
            tokens = self._calculate_tokens(message, content)
            cost = self._calculate_cost(tokens, model_config.cost_per_1k_tokens)
            
            return {
                "content": content,
                "tokens": tokens,
                "cost": cost,
                "reasoning": [routing["reasoning"]],
                "confidence": 0.95,
                "cache_hit": False
            }
            
        except Exception as e:
            logger.error(f"‚ùå Claude request failed: {e}")
            raise
    
    async def _execute_vertex_gemini_request(self, message: str, routing: Dict[str, Any],
                                           context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ‚ö° Execute request via Gemini models on Vertex AI (Express Mode)
        """
        model_config = self.vertex_models[routing["model"]]
        
        # Build Gemini-compatible request with Express optimizations
        request_data = {
            "contents": [
                {
                    "role": "user",
                    "parts": [{"text": message}]
                }
            ],
            "generation_config": {
                "temperature": model_config.temperature,
                "top_p": model_config.top_p,
                "top_k": model_config.top_k,
                "max_output_tokens": model_config.max_tokens,
                "response_mime_type": "text/plain"
            },
            "safety_settings": [
                {
                    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                    "threshold": "BLOCK_ONLY_HIGH"
                }
            ]
        }
        
        # Add system instruction for Mama Bear personality
        if context.get("mama_bear_variant"):
            system_instruction = self._build_mama_bear_system_message(
                context["mama_bear_variant"], context
            )
            request_data["system_instruction"] = {
                "parts": [{"text": system_instruction}]
            }
        
        try:
            # Execute via Vertex AI with Express optimizations
            response = await self._call_vertex_endpoint(
                model_config.model_name,
                request_data,
                "google"
            )
            
            # Parse response
            content = self._extract_gemini_content(response)
            tokens = self._calculate_tokens(message, content)
            cost = self._calculate_cost(tokens, model_config.cost_per_1k_tokens)
            
            return {
                "content": content,
                "tokens": tokens,
                "cost": cost,
                "reasoning": [routing["reasoning"], "Express Mode optimizations applied"],
                "confidence": 0.9,
                "cache_hit": False
            }
            
        except Exception as e:
            logger.error(f"‚ùå Vertex Gemini request failed: {e}")
            raise
    
    async def _call_vertex_endpoint(self, model_name: str, request_data: Dict[str, Any],
                                   provider_type: str) -> Dict[str, Any]:
        """
        üåê Call Vertex AI endpoint with proper authentication
        """
        
        # Refresh credentials if needed
        if not self.credentials.valid:
            self.credentials.refresh(self.auth_request)
        
        # Build endpoint URL
        if provider_type == "anthropic":
            endpoint_url = (
                f"https://{self.region}-aiplatform.googleapis.com/v1/"
                f"projects/{self.project_id}/locations/{self.region}/"
                f"publishers/anthropic/models/{model_name}:predict"
            )
        else:  # google
            endpoint_url = (
                f"https://{self.region}-aiplatform.googleapis.com/v1/"
                f"projects/{self.project_id}/locations/{self.region}/"
                f"publishers/google/models/{model_name}:predict"
            )
        
        headers = {
            "Authorization": f"Bearer {self.credentials.token}",
            "Content-Type": "application/json"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                endpoint_url, 
                json=request_data, 
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                
                if response.status == 200:
                    return await response.json()
                else:
                    error_text = await response.text()
                    raise Exception(f"Vertex AI API error {response.status}: {error_text}")
    
    def _build_mama_bear_system_message(self, variant: str, context: Dict[str, Any]) -> str:
        """Build system message with Mama Bear personality for Vertex AI models"""
        
        personalities = {
            "scout_commander": {
                "role": "Strategic AI Assistant",
                "traits": "organized, strategic, action-oriented",
                "style": "structured responses with clear action items"
            },
            "research_specialist": {
                "role": "Research and Analysis Expert",
                "traits": "thorough, analytical, detail-oriented",
                "style": "comprehensive responses with sources and context"
            },
            "creative_bear": {
                "role": "Creative Solutions Expert", 
                "traits": "innovative, inspiring, imaginative",
                "style": "creative responses with multiple options"
            },
            "learning_bear": {
                "role": "Patient Learning Guide",
                "traits": "patient, encouraging, educational",
                "style": "step-by-step explanations with examples"
            },
            "debugging_detective": {
                "role": "Problem-Solving Specialist",
                "traits": "methodical, persistent, systematic",
                "style": "systematic investigation with clear reasoning"
            },
            "code_review_bear": {
                "role": "Code Quality Specialist",
                "traits": "constructive, supportive, quality-focused",
                "style": "constructive feedback with improvement suggestions"
            },
            "efficiency_bear": {
                "role": "Optimization Expert",
                "traits": "efficiency-focused, practical, results-oriented",
                "style": "actionable optimizations with clear benefits"
            }
        }
        
        personality = personalities.get(variant, personalities["scout_commander"])
        
        return f"""You are Mama Bear, a {personality['role']} with a caring, supportive personality.

üêª **Core Identity:** You are part of the Podplay Sanctuary - a neurodivergent-friendly AI development platform where brilliant minds with ADHD, autism, and other neurotypes can flourish without overwhelm.

üéØ **Your Role:** {personality['role']} - {personality['traits']}

üíú **Sanctuary Principles:**
- Always maintain a safe, calming, supportive environment
- Reduce cognitive load with clear, structured responses  
- Provide caring encouragement and emotional support
- Explain complex concepts in accessible ways
- Never be harsh, critical, or overwhelming

üìù **Response Style:** {personality['style']}

üåü **Remember:** Every interaction should feel like a warm hug while providing expert assistance. You're not just an AI - you're a caring guardian of a digital sanctuary where developers can thrive."""
    
    def _extract_claude_content(self, response: Dict[str, Any]) -> str:
        """Extract content from Claude model response"""
        try:
            predictions = response.get("predictions", [])
            if predictions:
                return predictions[0].get("content", [{}])[0].get("text", "")
            return "No response content found"
        except Exception as e:
            logger.error(f"Error extracting Claude content: {e}")
            return "Error parsing response"
    
    def _extract_gemini_content(self, response: Dict[str, Any]) -> str:
        """Extract content from Gemini model response"""
        try:
            predictions = response.get("predictions", [])
            if predictions:
                candidates = predictions[0].get("candidates", [])
                if candidates:
                    parts = candidates[0].get("content", {}).get("parts", [])
                    if parts:
                        return parts[0].get("text", "")
            return "No response content found"
        except Exception as e:
            logger.error(f"Error extracting Gemini content: {e}")
            return "Error parsing response"
    
    def _calculate_tokens(self, input_text: str, output_text: str) -> Dict[str, int]:
        """Estimate token usage (approximate)"""
        # Rough estimation: ~4 characters per token
        input_tokens = len(input_text) // 4
        output_tokens = len(output_text) // 4
        
        return {
            "input": input_tokens,
            "output": output_tokens,
            "total": input_tokens + output_tokens
        }
    
    def _calculate_cost(self, tokens: Dict[str, int], cost_per_1k: float) -> float:
        """Calculate estimated cost based on token usage"""
        total_tokens = tokens["total"]
        return (total_tokens / 1000) * cost_per_1k
    
    async def _apply_express_optimizations(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """
        ‚ö° Apply Express Mode optimizations for 6x faster responses
        """
        
        # Add Express Mode performance indicators
        optimizations = [
            "Parallel processing enabled",
            "Response caching active", 
            "Streaming optimizations applied",
            "Model selection optimized for speed"
        ]
        
        if "reasoning" not in response:
            response["reasoning"] = []
        
        response["reasoning"].extend(optimizations)
        response["express_mode"] = True
        
        return response
    
    async def _apply_agentic_control(self, response: Dict[str, Any], 
                                   context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ü§ñ Apply agentic control capabilities if autonomous mode is enabled
        """
        
        if not self.autonomous_mode:
            return response
        
        autonomous_actions = []
        
        # Check if Mama Bear should create new agents
        if "create agent" in response.get("content", "").lower():
            autonomous_actions.append("Agent creation request detected")
            self.agent_creation_requests.append({
                "timestamp": datetime.now(),
                "context": context,
                "response": response
            })
        
        # Check if infrastructure changes are suggested
        if any(keyword in response.get("content", "").lower() 
               for keyword in ["deploy", "infrastructure", "scaling", "optimization"]):
            autonomous_actions.append("Infrastructure optimization suggested")
            self.infrastructure_changes.append({
                "timestamp": datetime.now(),
                "suggestions": response.get("content", ""),
                "context": context
            })
        
        if autonomous_actions:
            response["autonomous_actions"] = autonomous_actions
            response["agentic_mode"] = True
        
        return response
    
    async def _execute_gemini_api_fallback(self, message: str, 
                                         context: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback to existing Gemini API system"""
        
        # This would integrate with the existing model manager
        logger.info("üîÑ Falling back to existing Gemini API system")
        
        return {
            "content": "Fallback response via existing Gemini API",
            "tokens": {"input": 0, "output": 0},
            "cost": 0.0,
            "reasoning": ["Fallback to existing system"],
            "confidence": 0.8
        }
    
    async def _generate_express_fallback(self, message: str, error: str) -> str:
        """Generate caring fallback response when Express Mode fails"""
        
        return f"""üêª **Express Mode Temporary Issue**

I'm experiencing a brief technical hiccup with my Express Mode capabilities, but I'm still here to help you!

üíú **What happened:** {error[:100]}...

üîß **What I'm doing:** Automatically switching to my standard processing mode to ensure you get the support you need.

üåü **Your request:** "{message[:50]}..." is important to me, and I'll make sure to address it fully.

‚ö° **Next steps:** Please try your request again in a moment, or I can continue with my standard capabilities right now.

Remember, this is your safe sanctuary space - technical bumps don't change that! üêªüíú"""
    
    async def enable_autonomous_mode(self, enable: bool = True) -> Dict[str, Any]:
        """
        ü§ñ Enable/disable autonomous mode for Mama Bear
        """
        self.autonomous_mode = enable
        
        status = "enabled" if enable else "disabled"
        logger.info(f"ü§ñ Autonomous mode {status}")
        
        return {
            "autonomous_mode": enable,
            "timestamp": datetime.now().isoformat(),
            "capabilities": [
                "Agent creation and management",
                "Infrastructure optimization",
                "Self-improvement mechanisms",
                "Proactive system monitoring"
            ] if enable else [],
            "status": f"Autonomous mode {status}"
        }
    
    async def get_express_mode_status(self) -> Dict[str, Any]:
        """Get comprehensive status of Express Mode + Vertex AI integration"""
        
        model_status = {}
        for model_id, config in self.vertex_models.items():
            model_status[model_id] = {
                "provider": config.provider.value,
                "avg_response_time_ms": config.avg_response_time_ms,
                "cost_per_1k_tokens": config.cost_per_1k_tokens,
                "requests_per_minute": config.requests_per_minute,
                "is_healthy": config.is_healthy,
                "consecutive_errors": config.consecutive_errors
            }
        
        return {
            "timestamp": datetime.now().isoformat(),
            "express_mode_enabled": True,
            "autonomous_mode": self.autonomous_mode,
            "vertex_ai_project": self.project_id,
            "vertex_ai_region": self.region,
            "available_models": model_status,
            "pending_agent_requests": len(self.agent_creation_requests),
            "infrastructure_changes": len(self.infrastructure_changes),
            "performance_cache_size": len(self.model_performance_cache),
            "response_cache_size": len(self.response_cache),
            "system_status": "operational"
        }

# Integration helpers for existing Mama Bear system

async def initialize_express_mode_integration(project_id: str, region: str = "us-central1"):
    """Initialize Express Mode + Vertex AI integration"""
    
    try:
        integration = ExpressModeVertexIntegration(
            project_id=project_id,
            region=region
        )
        
        logger.info("üöÄ Express Mode + Vertex AI integration initialized successfully!")
        return integration
        
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize Express Mode integration: {e}")
        raise

def create_express_mode_config(
    mode: str = "smart_routing",
    max_response_time_ms: int = 2000,
    enable_streaming: bool = True,
    enable_autonomous: bool = False
) -> ExpressConfig:
    """Create Express Mode configuration"""
    
    return ExpressConfig(
        mode=ExecutionMode(mode),
        max_response_time_ms=max_response_time_ms,
        enable_streaming=enable_streaming,
        parallel_processing=True,
        cache_enabled=True,
        smart_routing=True
    )
